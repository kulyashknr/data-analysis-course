{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00eabddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faa3d0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 50\n",
    "strt = 0\n",
    "QUERY = \"nlp\"\n",
    "BASE_URL = f\"https://arxiv.org/search/?query={QUERY}&searchtype=all&abstracts=show&order=-announced_date_first&size={SIZE}&start={strt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e70ef9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9061e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.content)\n",
    "amountOfRes = list(soup.findAll(\"h1\"))[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "662f6a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page(soup):\n",
    "    links = soup.findAll(\"a\")\n",
    "    dict_publ = dict()\n",
    "    for link in links:\n",
    "        try:\n",
    "            features_link = link['href'].split(\"/\")\n",
    "    #         print(features_link)\n",
    "            if ('abs' == features_link[3]):\n",
    "    #         'ps' == features_link[3]) or ('format' == features_link[3]):\n",
    "                dict_publ[features_link[-1]] = link['href'] \n",
    "    #             print(features_link[-1])\n",
    "    #             print(link['href'])\n",
    "        except:\n",
    "            pass\n",
    "    #         print('This link has no href')\n",
    "    return dict_publ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8a7e4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "amount = int(amountOfRes.split(\" \")[3].replace(\",\",\"\"))\n",
    "pages_amount = amount/SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1c81556",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = parse_page(soup)\n",
    "# print(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cd4c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|███████████▏                               | 23/88 [00:39<01:37,  1.50s/it]"
     ]
    }
   ],
   "source": [
    "for strt in tqdm(range(1,math.ceil(pages_amount))):\n",
    "    BASE_URL = f\"https://arxiv.org/search/?query={QUERY}&searchtype=all&abstracts=show&order=-announced_date_first&size={SIZE}&start={strt*SIZE}\"\n",
    "    res = requests.get(BASE_URL)\n",
    "    if res.status_code==200:\n",
    "        soup = BeautifulSoup(res.content)\n",
    "        papers.update(parse_page(soup))\n",
    "    else:\n",
    "        print(BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38ce4299",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'QUERY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hz/jm00xhqx2v5g2zp5stb9zf000000gn/T/ipykernel_28004/3426994787.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{QUERY}.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpapers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'QUERY' is not defined"
     ]
    }
   ],
   "source": [
    "with open(f\"{QUERY}.json\",\"w\") as fh:\n",
    "    json.dump(papers,fh,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "785af221",
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper in list(papers.items())[:1]:\n",
    "    paper_url = paper[1]\n",
    "    res = requests.get(paper_url)\n",
    "    soup = BeautifulSoup(res.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "51c3fb5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buff = soup.select(\"#abs > h1\")\n",
    "el = str(buff[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cda50172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Efficacy of Transformer Networks for Classification of Raw EEG Data'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind1 = el.find(\"</span>\")\n",
    "el[ind1+7:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "407a35a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Submitted on 8 Feb 2022]\n"
     ]
    }
   ],
   "source": [
    "buff = soup.select(\"#abs > div.dateline\")\n",
    "el = str(buff[0].text)\n",
    "print(el.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "948bff07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authors:Gourav Siddhad, Anmol Gupta, Debi Prosad Dogra, Partha Pratim Roy\n"
     ]
    }
   ],
   "source": [
    "buff = soup.select(\"#abs > div.authors\")\n",
    "el = str(buff[0].text)\n",
    "print(el.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4a136ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract:  With the unprecedented success of transformer networks in natural language\n",
      "processing (NLP), recently, they have been successfully adapted to areas like\n",
      "computer vision, generative adversarial networks (GAN), and reinforcement\n",
      "learning. Classifying electroencephalogram (EEG) data has been challenging and\n",
      "researchers have been overly dependent on pre-processing and hand-crafted\n",
      "feature extraction. Despite having achieved automated feature extraction in\n",
      "several other domains, deep learning has not yet been accomplished for EEG. In\n",
      "this paper, the efficacy of the transformer network for the classification of\n",
      "raw EEG data (cleaned and pre-processed) is explored. The performance of\n",
      "transformer networks was evaluated on a local (age and gender data) and a\n",
      "public dataset (STEW). First, a classifier using a transformer network is built\n",
      "to classify the age and gender of a person with raw resting-state EEG data.\n",
      "Second, the classifier is tuned for mental workload classification with open\n",
      "access raw multi-tasking mental workload EEG data (STEW). The network achieves\n",
      "an accuracy comparable to state-of-the-art accuracy on both the local (Age and\n",
      "Gender dataset; 94.53% (gender) and 87.79% (age)) and the public (STEW dataset;\n",
      "95.28% (two workload levels) and 88.72% (three workload levels)) dataset. The\n",
      "accuracy values have been achieved using raw EEG data without feature\n",
      "extraction. Results indicate that the transformer-based deep learning models\n",
      "can successfully abate the need for heavy feature-extraction of EEG data for\n",
      "successful classification.\n"
     ]
    }
   ],
   "source": [
    "buff = soup.select(\"#abs > blockquote\")\n",
    "el = str(buff[0].text)\n",
    "print(el.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a2c80604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal Processing (eess.SP); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)\n"
     ]
    }
   ],
   "source": [
    "buff = soup.select(\"#abs td.tablecell.subjects\")\n",
    "# print(buff)\n",
    "el = str(buff[0].text)\n",
    "print(el.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "599d9b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#abs > div.metatable > table > tbody > tr:nth-child(1) > td.tablecell.subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "68bb6d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pdf/2202.05170\n"
     ]
    }
   ],
   "source": [
    "buff = soup.select(\"#abs > a\")\n",
    "el = str(buff[0][\"href\"])\n",
    "print(el.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcae9e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "body > main > div.layout__container.main-col.a-item > div > div.offer__container > div.offer__sidebar > div.offer__advert-info > div.offer__sidebar-header > div"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
